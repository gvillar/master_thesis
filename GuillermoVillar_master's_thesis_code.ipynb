{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master's thesis code\n",
    "\n",
    "Code for the Master's thesis in <a href=\"https://www.tilburguniversity.edu/education/masters-programmes/data-science-and-society\" target=\"_blank\">Data Science & Society</a> at Tilburg University (The Netherlands), inspired from the paper <a href=\"https://arxiv.org/abs/1907.00400\" target=\"_blank\">\"Prediction is very hard, especially about conversion. Predicting user purchases from clickstream data in fashion e-commerce\" (Bigon et al., 2019)</a>, and only possible thanks to it. Work supervised by Dr. Giovanni Cassani.\n",
    "\n",
    "Info about the thesis will be available when it is publicly accessible through the university archive.\n",
    "\n",
    "\n",
    "### Copyright and License\n",
    "\n",
    "&copy; 2020 Guillermo Villar. All rights reserved.\n",
    "\n",
    "(Copyright 2020 Guillermo Villar. All rights reserved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "import csv\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps for refining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothetic names to include events and dwell times\n",
    "session_cl = pd.read_csv(\"ev.csv\")\n",
    "session_dw = pd.read_csv(\"dwe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that values are floats\n",
    "for arr in np.array(session_dw):\n",
    "    for value in arr:\n",
    "        if isinstance(value,float) == False:\n",
    "            print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As both columns will be merged, we don't need the column for the labels twice\"\"\"\n",
    "# deleting the column of the label not to have it duplicated in the merged dataset\n",
    "session_cl.pop(\"label_cl\")\n",
    "\n",
    "# merging both datasets\n",
    "session_merged = pd.concat([session_cl, session_dw], axis=1)\n",
    "session_merged.rename(columns = lambda x: int(x) if x != \"label_dw\" else x, inplace = True)\n",
    "session_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we need to sort the numeric values naming the columns to have the sequence like this: \"event - dwell time - event...\"\"\"\n",
    "# sorting numeric values, with label as the last column \n",
    "sorted_session_col = sorted(session_merged.columns[:-1], key = int)\n",
    "sorted_session_col.append(\"label_dw\")\n",
    "print(sorted_session_col)\n",
    "\n",
    "# reindexing the columns based on this order\n",
    "session_merged = session_merged.reindex(sorted_session_col, axis=1)\n",
    "session_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df = session_merged[session_merged['label_dw'] == 1]\n",
    "nobuy_df = session_merged[session_merged['label_dw'] == 0]\n",
    "nobuy_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final steps for refining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These steps have been omitted since they refer to the specifities of the datasets. Only the steps for the combination of both datasets are shown (following piece of code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combining datasets\"\"\"\n",
    "final_df = pd.merge(buy_df, nobuy_down, how='outer')\n",
    "\n",
    "# checking the merged dataset corresponds to twice the rows and the same columns\n",
    "print(final_df.shape[0] == nobuy_down.shape[0] * 2)\n",
    "print(final_df.shape[1] == nobuy_down.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df[\"label_dw\"] == 0].iloc[:,30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the dataset to make the quantiles based on the training set, not on the whole dataset\"\"\"\n",
    "x = final_df.iloc[:,:-1]\n",
    "y = final_df.iloc[:,-1]\n",
    "# splitting in train, val and test set to bin the dataset ONLY with the dwell times from the train set\n",
    "x_train, x_testval, y_train, y_testval = train_test_split(x, y, stratify = y, test_size = 0.3, random_state= 333)\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[range(1, x_train.shape[1], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"With the list of dwell times, we can make the customized quantiles for the dataset\"\"\"\n",
    "# taking the dwell times from the training set\n",
    "nested_dw = list(np.array(x_train[range(1, x_train.shape[1], 2)]))\n",
    "# ignoring cells after more info is not provided (dw = -1000000)\n",
    "list_dw = [j for i in nested_dw for j in i if j != -1000000]\n",
    "print(list_dw[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists of dwelltimes for each sort of sequences to compute quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing and saving dataframe with quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The functions s.quantile and pd.cut are combined to set the limits for the quantiles\n",
    "and apply this personalized partition for the binning process in quantiles, respectively.\"\"\"\n",
    "\n",
    "\"\"\"k represents the number of quantiles\"\"\"\n",
    "# starting from 1 to compute the 'division' variable\n",
    "for k in range(1,5):\n",
    "    division = list(np.arange(0,1,1/k))\n",
    "    if len(division) > k:\n",
    "        division = division[:-1]\n",
    "    # using s.quantile in the list of every dwell time to set which dwell times match the divisions of the quantiles\n",
    "    part = list(pd.Series(list_dw).quantile(division))\n",
    "    \"\"\"Fictional boundary towards unexpected values: min -1, max 9999\"\"\"\n",
    "    part.append(9999)\n",
    "    partition = [x if x != 0 else -1 for x in part]\n",
    "    print(partition)\n",
    "\n",
    "    \"\"\"Creating a variable to use the resulting dataset for the modifications through bins\"\"\"\n",
    "    # final_df.iloc[:,:-1] is the equivalent of x. x not typed because x_k would also affect x\n",
    "    x_k = final_df.iloc[:,:-1]\n",
    "    # applying pd.cut with the personalized s.quantile bins\n",
    "    for i in range(1, x_k.shape[1], 2):\n",
    "        # labels as strings because it is the way to force the number to be a false integer in the future \n",
    "        x_k.iloc[:,i] = pd.cut(x_k.iloc[:,i], bins = partition, labels = [str(i+1) for i in range(k)])\n",
    "    print(range(k), \"partition\", partition, \"k\", k)\n",
    "        \n",
    "    \"\"\"EXPERIMENT 1. Dataset 1. Events + dwell times\"\"\"\n",
    "    # first EOS\n",
    "    x_k = x_k.replace(np.nan, 0)\n",
    "    x_k = x_k.replace(\"_\", 0)\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_dwell_times_and_events\", np.array(x_k))\n",
    "    \n",
    "    \"\"\"EXPERIMENT 1. Dataset 2. Only events\"\"\"\n",
    "    x_k_nodw = x_k.iloc[:, np.arange(0,x_k.shape[1], 2)]\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_only_events\", np.array(x_k_nodw))\n",
    "    \n",
    "    \"\"\"EXPERIMENT 1. Dataset 3. Only dwell times\"\"\"\n",
    "    x_k_onlydw = x_k.iloc[:, np.arange(1,x_k.shape[1], 2)]\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_only_dwell_times\", np.array(x_k_onlydw))\n",
    "    \n",
    "    \"\"\"EXPERIMENT 2. Dataset 4. Mixed features\"\"\"\n",
    "    x_k_mixed = []\n",
    "    for arr in np.array(x_k):\n",
    "        mixed_arr = np.array([str(arr[i]) + str(arr[i+1]) for i in range(0, len(arr), 2)])\n",
    "        refined_arr = np.array([mixed if '0' not in mixed else '0' for mixed in mixed_arr])\n",
    "        x_k_mixed.append(refined_arr)\n",
    "    x_k_mixed = pd.DataFrame(np.array(x_k_mixed))\n",
    "    mixed_features = np.unique(np.array(x_k_mixed))\n",
    "    # -1 because 0 will not be converted to a new symbol\n",
    "    names_mixed = string.ascii_lowercase[:len(mixed_features)-1]\n",
    "    names_mixed = \"0\" + names_mixed\n",
    "    dict_features = dict(zip(mixed_features, names_mixed))\n",
    "    x_k_mixed = x_k_mixed.replace(dict_features)\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_mixed_features\", np.array(x_k_mixed))\n",
    "    \n",
    "    \"\"\"EXPERIMENT 2. Dataset 5. Event unrolling\"\"\"\n",
    "    x_k_unrolled = []\n",
    "    for arr in np.array(x_k):\n",
    "        unrolled_arr = np.array([arr[i] * int(arr[i+1]) for i in range(0, len(arr), 2) if arr[i+1] != 0])\n",
    "        split_arr = np.array(list(\"\".join(unrolled_arr)))\n",
    "        padded_arr = np.pad(split_arr, (0, k*(x_k_nodw.shape[1]) - len(split_arr)), \"constant\", constant_values=\"0\")\n",
    "        x_k_unrolled.append(padded_arr)\n",
    "    x_k_unrolled = pd.DataFrame(np.array(x_k_unrolled))\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_unrolled_events\", np.array(x_k_unrolled))\n",
    "    \n",
    "    \"\"\"EXPERIMENT 3. Dataset 6. Shuffled dwell times\"\"\"\n",
    "    x_k_shuffled = []\n",
    "    rd.seed(500)\n",
    "    for arr in np.array(x_k):\n",
    "        shuffled_arr = np.array([arr[i] if (arr[i] == 0 or str(arr[i]).isdigit() == False) else str(rd.randint(1,k)) for i in range(len(arr))])\n",
    "        x_k_shuffled.append(shuffled_arr)\n",
    "    x_k_shuffled = pd.DataFrame(np.array(x_k_shuffled))\n",
    "    np.savez_compressed(\"x_\" + str(k) + \"_shuffled_dwell_times_and_events\", np.array(x_k_shuffled))\n",
    "\n",
    "    \n",
    "# y saved as a variable\n",
    "y.to_csv('y.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the created datasets for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dataset with dwell times between events\")\n",
    "x_k.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset with only events\")\n",
    "x_k_nodw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset with only dwell times\")\n",
    "x_k_onlydw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dataset with mixed features\")\n",
    "x_k_mixed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset with unrolled events\")\n",
    "x_k_unrolled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dataset with shuffled dwell times between events\")\n",
    "x_k_shuffled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization about dwell times\"\"\"\n",
    "# extracting dwell times for each sort of sequences\n",
    "counts_dw_buy = np.array([value for session in np.array(buy_df.iloc[:,:-1]) for value in session if (isinstance(value, str) == False and value != -1000000)])\n",
    "counts_dw_nobuy = np.array([value for session in np.array(nobuy_down.iloc[:,:-1]) for value in session if (isinstance(value, str) == False and value != -1000000)])\n",
    "# plotting\n",
    "counts_b, bins_b, bars_b = plt.hist(counts_dw_buy, 100, alpha = 0.5, weights = np.zeros_like(counts_dw_buy) + 1 / len(counts_dw_buy),  label=\"buy\")\n",
    "counts_nb, bins_nb, bars_nb = plt.hist(counts_dw_nobuy, 100, alpha = 0.5,  weights = np.zeros_like(counts_dw_nobuy) + 1 / len(counts_dw_nobuy), label=\"no buy\")\n",
    "plt.legend(loc= \"upper right\")\n",
    "plt.title(\"Relative frequencies of dwell times\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization about the medians of dwell times\"\"\"\n",
    "# extracting dwell times for each sort of sequences\n",
    "medians = []\n",
    "for session in np.array(final_df.iloc[:,:-1]):\n",
    "    median_session = np.median([value for value in session if (isinstance(value, str) == False and value != -1000000)])\n",
    "    medians.append(int(median_session))\n",
    "\n",
    "# plotting\n",
    "plt.hist(medians, 100, alpha = 0.5)\n",
    "plt.title(\"Medians of dwell times from every sequence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization about the bins of dwell times\"\"\"\n",
    "# bins of dwell times\n",
    "xy_k = pd.concat([x_k, y], axis = 1)\n",
    "xy_buy = xy_k[xy_k[\"label_dw\"] == 1].iloc[:,:-1]\n",
    "xy_nobuy = xy_k[xy_k[\"label_dw\"] == 0].iloc[:,:-1]\n",
    "# counting the types of bins for each sort of sequences\n",
    "xy_buy_counts = Counter(np.array([session[i] for session in np.array(xy_buy) for i in range(1,len(session),2) if session[i] != 0]))\n",
    "xy_nobuy_counts = Counter(np.array([session[i] for session in np.array(xy_nobuy) for i in range(1,len(session),2) if session[i] != 0]))\n",
    "# percentages of the types of bins for each sort of sequences\n",
    "xy_buy_perc = {key : xy_buy_counts[key] / sum(xy_buy_counts.values()) for key in sorted(xy_buy_counts.keys())}\n",
    "xy_nobuy_perc = {key : xy_nobuy_counts[key] / sum(xy_nobuy_counts.values()) for key in sorted(xy_nobuy_counts.keys())}\n",
    "xy_perc = {\"Buy\": xy_buy_perc, \"No buy\": xy_nobuy_perc}\n",
    "# plotting\n",
    "pd.DataFrame(xy_perc).T.plot.barh(stacked=True, align='center', colors = ['#00ce00','#009a00','#006700','#003400'])\n",
    "print(pd.DataFrame(xy_perc))\n",
    "ax = plt.subplot()\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
    "          fancybox=True, shadow=False, ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FINAL PREPROCESSING FOR NAIVE BAYES, MARKOV CHAINS AND LSTM\"\"\"\n",
    "for type_dataframe in [\"dwell_times_and_events\", \"only_events\", \"only_dwell_times\",\n",
    "                       \"mixed_features\",\"unrolled_events\",\"shuffled_dwell_times_and_events\"]:\n",
    "    for k in range(1,5):\n",
    "        saved_arr =  np.load(\"x_\" + str(k) + \"_\" + type_dataframe + \".npz\")[\"arr_0\"].astype(\"str\")\n",
    "        array_k = []\n",
    "        for arr in saved_arr:\n",
    "            new_arr = np.array([value if value != \"0\" else \"_\" for value in arr])\n",
    "            array_k.append(new_arr)\n",
    "        array_k = np.array(array_k)\n",
    "        print(array_k)\n",
    "        corpus_k = [\"\".join(seq) for seq in array_k]\n",
    "        for n in range(1,6):\n",
    "            # BOS added while working on the algorithms (NB and MC)\n",
    "            bos_symbol = \"#\"\n",
    "            bos_corpus = [\"#\"*(n-1) + seq for seq in corpus_k]\n",
    "            ngram_corpus = []\n",
    "            for seq in bos_corpus:\n",
    "                # adding EOS symbols corresponding to the ngrams\n",
    "                ngram_seq = [seq[i:n+i] for i in range(len(seq)-n)]\n",
    "                ngram_corpus.append(ngram_seq)\n",
    "            np.savez_compressed(str(n) + \"gram_\" + str(k) + \"bins\" + \"_\" + type_dataframe, ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# giving y_nb the same format than the xbags\n",
    "y_nb = np.array(list(pd.read_csv('y.csv').iloc[:,0]))\n",
    "\n",
    "\"\"\"These lists will store every result from the three types of df: with dw, without dw and with dw shuffled\"\"\"\n",
    "all_nb_train, all_nb_val, all_nb_test = [], [], []\n",
    "\n",
    "for type_dataframe in [\"dwell_times_and_events\", \"only_events\", \"only_dwell_times\",\n",
    "                       \"mixed_features\",\"unrolled_events\",\"shuffled_dwell_times_and_events\"]:\n",
    "    \n",
    "    \"\"\"These lists will store every result from all the ngrams with all the k from a dataframe\"\"\"\n",
    "    df_nb_train, df_nb_val, df_nb_test = [], [], []\n",
    "    \n",
    "    for k in range(1,5):\n",
    "        \n",
    "        \"\"\"These lists will store every result from all the ngrams with a specific k dataframe\"\"\"\n",
    "        nb_train, nb_val, nb_test = [], [], []\n",
    "        \n",
    "        for n in range(2,6):\n",
    "\n",
    "            # reading the array with ngrams and removing ngrams with only zeros\n",
    "            xnb = np.array(pd.DataFrame(np.load(str(n) + \"gram_\" + str(k) + \"bins\" + \"_\" + type_dataframe + \".npz\")[\"arr_0\"]))\n",
    "            x_nb = []\n",
    "            for arr in xnb:\n",
    "                new_arr = np.array([value for value in arr if set(value) != {\"_\"}])\n",
    "                x_nb.append(new_arr)\n",
    "            x_nb = np.array(x_nb)\n",
    "            \n",
    "            # train and the rest\n",
    "            x_nb_train, x_nb_testval, y_nb_train, y_nb_testval = train_test_split(x_nb, y_nb, stratify = y_nb, test_size = 0.3, random_state= 333)\n",
    "            # train, val, test (0.3 * 0.5 is 0.15)\n",
    "            x_nb_val, x_nb_test, y_nb_val, y_nb_test = train_test_split(x_nb_testval, y_nb_testval, stratify = y_nb_testval, test_size = 0.5, random_state=400)\n",
    "\n",
    "        \n",
    "            \"\"\"FINAL PREPROCESSING FOR NB\"\"\"\n",
    "            x_nb_train, x_nb_val, x_nb_test = list(x_nb_train), list(x_nb_val), list(x_nb_test)\n",
    "            \n",
    "            # converting into dict for b-o-ngrams\n",
    "            train_dict = np.array([Counter(x_nb_train[i]) for i in range(len(x_nb_train))])\n",
    "            val_dict = np.array([Counter(x_nb_val[i]) for i in range(len(x_nb_val))])\n",
    "            test_dict = np.array([Counter(x_nb_test[i]) for i in range(len(x_nb_test))])\n",
    "            \n",
    "            vec = DictVectorizer()\n",
    "            tfidf = TfidfTransformer()\n",
    "            xbag_train = []\n",
    "            xbagtrain = vec.fit_transform(train_dict).toarray()\n",
    "            for session in xbagtrain:\n",
    "                ones_session = np.array([1 if value != 0 else 0 for value in session])\n",
    "                xbag_train.append(ones_session)\n",
    "            xbag_train = np.array(xbag_train)\n",
    "            \n",
    "            xbag_val = []\n",
    "            xbagval = vec.transform(val_dict).toarray()\n",
    "            for session in xbagval:\n",
    "                ones_session = np.array([1 if value != 0 else 0 for value in session]) \n",
    "                xbag_val.append(ones_session) \n",
    "            xbag_val = np.array(xbag_val)\n",
    "            \n",
    "            xbag_test = []\n",
    "            xbagtest = vec.transform(test_dict).toarray()\n",
    "            for session in xbagtest:\n",
    "                ones_session = np.array([1 if value != 0 else 0 for value in session])\n",
    "                xbag_test.append(ones_session)\n",
    "            xbag_test = np.array(xbag_test)\n",
    "\n",
    "            \"\"\"Setting the Bernoulli Naive Bayes\"\"\"\n",
    "            nb = BernoulliNB()\n",
    "            nb.fit(xbag_train, y_nb_train)\n",
    "            \n",
    "           # adding accuracy to the terciary subgroups of lists\n",
    "            ybag_pred_tr = nb.predict(xbag_train)\n",
    "            nb_train.append(accuracy_score(y_nb_train, ybag_pred_tr))\n",
    "            ybag_pred_val = nb.predict(xbag_val)\n",
    "            nb_val.append(accuracy_score(y_nb_val, ybag_pred_val))\n",
    "            ybag_pred_test = nb.predict(xbag_test)\n",
    "            nb_test.append(accuracy_score(y_nb_test, ybag_pred_test))\n",
    "            \n",
    "        # adding these lists to the secondary subgroups of lists\n",
    "        df_nb_train.append(nb_train)\n",
    "        df_nb_val.append(nb_val)\n",
    "        df_nb_test.append(nb_test)\n",
    "        \n",
    "    \"\"\"Visualization of the results\"\"\"\n",
    "    nb_viz = pd.DataFrame(df_nb_test, columns = [\"2grams\", \"3grams\", \"4grams\", \"5grams\"], index = [\"1 bin\", \"2 bins\", \"3 bins\", \"4 bins\"])\n",
    "    ax = plt.axes()\n",
    "    heatmap = sns.heatmap(nb_viz, annot=True, cmap=\"Blues\", vmin=0.50, vmax=1, ax = ax)\n",
    "    plt.title(\"Dataset with \" + type_dataframe.replace(\"_\",\" \"))\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # adding these lists to the primary subgroups of lists\n",
    "    all_nb_train.append(df_nb_train)\n",
    "    all_nb_val.append(df_nb_val)\n",
    "    all_nb_test.append(df_nb_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Final visualization\"\"\"\n",
    "types_dataframes = [\"Dwell times between events\", \"Only events\", \"Only dwell times\",\n",
    "                       \"Mixed features\",\"Unrolled events\",\"Shuffled dwell times between events\"]\n",
    "\n",
    "lollipop_nb = pd.DataFrame(np.array(all_nb_test)[:,:,3], columns = [\"1 bin\", \"2 bins\", \"3 bins\", \"4 bins\"], index = [\"dataset 1\", \"dataset 2\", \"dataset 3\", \"dataset 4\", \"dataset5\", \"dataset 6\"])\n",
    "plt.hlines(y=range(6), xmin=lollipop_nb[\"1 bin\"], xmax=lollipop_nb[\"4 bins\"], color='grey', alpha = 0.5)\n",
    "plt.plot(lollipop_nb['1 bin'], types_dataframes, \"o\", alpha = 0)\n",
    "plt.scatter(lollipop_nb['4 bins'], types_dataframes, color='#00ce00', label='4 bins')\n",
    "plt.scatter(lollipop_nb['3 bins'], types_dataframes, color='#009a00', label='3 bins')\n",
    "plt.scatter(lollipop_nb['2 bins'], types_dataframes, color='#006700', label='2 bins')\n",
    "plt.scatter(lollipop_nb['1 bin'], types_dataframes, color='#003400', label='1 bin')\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1)\n",
    " \n",
    "# adding title and axis names\n",
    "plt.title(\"Accuracy with Naive Bayes\", loc='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the essential parts and the results of the Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"latest bag of ngrams in the loop\"\"\"\n",
    "# all the bag\n",
    "print(\"bag of ngrams of dataset with\" + \"shuffled\" + \" and \" + str(n) + \"grams\", xbag_test)\n",
    "# bag shape\n",
    "print(\"shape\", xbag_test.shape)\n",
    "# encoding of the first session\n",
    "print(\"first session encoded\", xbag_test[0])\n",
    "# 100 first examples of feature names\n",
    "nb_feats = list(vec.get_feature_names())\n",
    "print(\"100 examples of features\", nb_feats[:100])\n",
    "\n",
    "all_feats = list(np.unique([ngram for session in x_nb for ngram in session]))\n",
    "train_feats = list(np.unique([ngram for session in x_nb_train for ngram in session]))\n",
    "\n",
    "# this will return \"False\" because the features from the bag of ngrams do not take the vocabulary from the whole dataset\n",
    "print(list(all_feats) == nb_feats)\n",
    "\n",
    "# this will return \"True\" because the features from the bag of ngrams only take the vocabulary from the training set, as it must be\n",
    "print(list(train_feats) == nb_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed(\"results_nb\", np.array(all_nb_test))\n",
    "print(\"raw results with dwell times between events\\n\", np.array(all_nb_test[0]), \"\\n\")\n",
    "print(\"raw results with only events\\n\", np.array(all_nb_test[1]), \"\\n\")\n",
    "print(\"raw results with only dwell times\\n\", np.array(all_nb_test[2]), \"\\n\")\n",
    "print(\"raw results with mixed features\\n\", np.array(all_nb_test[3]), \"\\n\")\n",
    "print(\"raw results with unrolled events\\n\", np.array(all_nb_test[4]), \"\\n\")\n",
    "print(\"raw results with shuffled dwell times between events\\n\", np.array(all_nb_test[5]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ymc = np.array(list(pd.read_csv('y.csv').iloc[:,0]))\n",
    "\n",
    "\"\"\"These lists will store every result from the three types of df: with dw, without dw and with dw shuffled\"\"\"\n",
    "all_mc_train, all_mc_val, all_mc_test = [], [], []\n",
    "\n",
    "for type_dataframe in [\"dwell_times_and_events\", \"only_events\", \"only_dwell_times\",\n",
    "                       \"mixed_features\",\"unrolled_events\",\"shuffled_dwell_times_and_events\"]:\n",
    "    \n",
    "    \"\"\"These lists will store every result from all the ngrams with all the k from a dataframe\"\"\"\n",
    "    df_mc_train, df_mc_val, df_mc_test = [], [], []\n",
    "    \n",
    "    for k in range(1,5):\n",
    "        \n",
    "        \"\"\"These lists will store every result from all the ngrams with a specific k dataframe\"\"\"\n",
    "        mc_train, mc_val, mc_test = [], [], []\n",
    "        \n",
    "        # transition probabilities don't work on unigrams\n",
    "        for n in range(2,6):\n",
    "            \n",
    "            \"\"\"PART 1. RELEVANT INFORMATION TO FEED THE ALGO (TRANSITION PROBABILITY MATRIX) AND TEST IT\"\"\"\n",
    "            # ngrams (n from 2 to 5) sequences created before to associate ngrams to the transition prob matrix\n",
    "            xmc = pd.DataFrame(np.load(str(n) + \"gram_\" + str(k) + \"bins\" + \"_\" + type_dataframe + \".npz\")[\"arr_0\"])\n",
    "            # train, val, test (to associate them to the divisions above)\n",
    "            xmc_train, xmc_valte, ymc_train, ymc_valte = train_test_split(xmc, ymc, stratify = ymc, test_size = 0.3, random_state = 333)\n",
    "            xmc_val, xmc_test, ymc_val, ymc_test = train_test_split(xmc_valte, ymc_valte, stratify = ymc_valte, test_size = 0.5, random_state = 400)               \n",
    "            # to retrieve each sort of sequences under this split\n",
    "            xmc_all = pd.concat([pd.DataFrame(np.array(xmc_train)), pd.DataFrame(np.array(ymc_train))], axis = 1)\n",
    "            # to create the transition probability matrix on buy sequences\n",
    "            xmc_buy = np.array(xmc_all[xmc_all.iloc[:,-1] == 1].iloc[:,:-1])\n",
    "            # to create the transition probability matrix on no-buy sequences\n",
    "            xmc_nobuy = np.array(xmc_all[xmc_all.iloc[:,-1] == 0].iloc[:,:-1])\n",
    "\n",
    "            \"\"\"PART 2. ALL POSSIBLE COMBINATIONS FROM THE BIGRAMS VERSION OF THE TRAINING SET TO GENERATE THE WHOLE VOCAB\"\"\"\n",
    "            # this only uses the training set with bigrams to exploit the possible combinations that there could be in higher ngrams\n",
    "            if n == 2:\n",
    "                unique_bigrams = list(np.unique(np.array(xmc_train)))\n",
    "                # adding the following because bigger ngrams have also two or more bos and eos\n",
    "                unique_bigrams.extend([\"##\", \"__\"])\n",
    "                # unique unigrams to make all the possible combinations\n",
    "                unique_features = list(np.unique([symbol for bigram in unique_bigrams for symbol in bigram]))\n",
    "                # all the possible combinations from unigrams up to 5 symbols together\n",
    "                all_combinations = list(itertools.product(unique_features, unique_features, unique_features, unique_features, unique_features))\n",
    "                all_combinations = [\"\".join(combination) for combination in all_combinations]\n",
    "            \n",
    "            \"\"\"Now we will use these possible combinations for our ngram model\"\"\"\n",
    "            # cutting these possible combinations according to the size of the ngram\n",
    "            all_ngrams = np.unique([combination[:n] for combination in all_combinations])\n",
    "            initialization_transitions = []\n",
    "            for combination in all_ngrams:\n",
    "                counter = 0\n",
    "                for i in range(1,len(combination)):\n",
    "                    if combination[i-1] + combination[i] not in unique_bigrams:\n",
    "                        counter += 1\n",
    "                if counter == 0 and (set(combination) not in [{\"_\"},{\"#\"}]):\n",
    "                    if (combination.startswith(\"#\") and combination.endswith(\"_\")) == False:\n",
    "                        initialization_transitions.append(combination)\n",
    "            initialization_transitions = [np.array(initialization_transitions)]\n",
    "            \n",
    "            \"\"\"PART 3. TRANSITION PROBABILITIES OF N-ORDER MARKOV CHAIN\"\"\"\n",
    "            # initialized transitions + real counts\n",
    "            all_count_dicts = []\n",
    "            for count in [[initialization_transitions, xmc_nobuy], [initialization_transitions, xmc_buy]]:\n",
    "                count_dict = dict()\n",
    "                for type_count in count:\n",
    "                    for session in type_count:    \n",
    "                        for key in session:\n",
    "                            i = key[:-1]\n",
    "                            j = key[-1]\n",
    "                            if i in count_dict:\n",
    "                                if j in count_dict[i]:\n",
    "                                    count_dict[i][j] += 1\n",
    "                                else:\n",
    "                                    count_dict[i][j] = 1\n",
    "                            else:\n",
    "                                count_dict[i] = {j : 1}\n",
    "                all_count_dicts.append(count_dict)\n",
    "\n",
    "            # probability matrix\n",
    "            for type_count in all_count_dicts:\n",
    "                for key in type_count:\n",
    "                    key_sum = sum(type_count[key].values())\n",
    "                    for subkey in type_count[key]:\n",
    "                        type_count[key][subkey] = type_count[key][subkey]/key_sum\n",
    "\n",
    "            \"\"\"PART 3. COMPUTE THE TRANSITIONS IN THE SEQUENCES\"\"\"\n",
    "            \n",
    "            prior = np.log(0.5)\n",
    "            all_mc_pred = []\n",
    "            for subset in [xmc_train, xmc_val, xmc_test]:\n",
    "                pred_mc = []\n",
    "                for session in np.array(subset):\n",
    "                    prod_buy = []\n",
    "                    prod_nobuy = []\n",
    "                    for ng in session:\n",
    "                        key = ng[:-1]\n",
    "                        subkey = ng[-1]\n",
    "                        try:\n",
    "                            prod_nobuy.append(np.log(all_count_dicts[0][key][subkey]))\n",
    "                        except KeyError:\n",
    "                            prod_nobuy.append(1)\n",
    "                        try:\n",
    "                            prod_buy.append(np.log(all_count_dicts[1][key][subkey]))\n",
    "                        except KeyError:\n",
    "                            prod_buy.append(1)\n",
    "                            \n",
    "                    likelihood_nobuy = np.sum(np.array(prod_nobuy))\n",
    "                    # print(likelihood_nobuy)\n",
    "                    likelihood_buy = np.sum(np.array(prod_buy))\n",
    "                    # print(likelihood_buy)\n",
    "                    numerator_buy = np.exp(likelihood_buy + prior)\n",
    "                    numerator_nobuy = np.exp(likelihood_nobuy + prior)\n",
    "                    denominator = numerator_buy + numerator_nobuy\n",
    "                    perc_buy = (numerator_buy / denominator)\n",
    "                    perc_nobuy = (numerator_nobuy / denominator)\n",
    "                    # print(perc_nobuy, perc_buy)\n",
    "                    guess_mc = np.argmax([perc_nobuy, perc_buy])\n",
    "                    pred_mc.append(guess_mc)\n",
    "                pred_mc = np.array(pred_mc)\n",
    "                all_mc_pred.append(pred_mc)\n",
    "\n",
    "            # adding accuracy to the terciary subgroups of lists\n",
    "            mc_train.append(accuracy_score(ymc_train, all_mc_pred[0]))\n",
    "            mc_val.append(accuracy_score(ymc_val, all_mc_pred[1]))\n",
    "            mc_test.append(accuracy_score(ymc_test, all_mc_pred[2]))\n",
    "            \n",
    "        # adding these lists to the secondary subgroups of lists\n",
    "        df_mc_train.append(mc_train)\n",
    "        df_mc_val.append(mc_val)\n",
    "        df_mc_test.append(mc_test)\n",
    "    \n",
    "    \"\"\"Visualization of the results\"\"\"\n",
    "    mc_viz = pd.DataFrame(df_mc_test, columns = [\"2grams\", \"3grams\", \"4grams\", \"5grams\"], index = [\"1 bin\", \"2 bins\", \"3 bins\", \"4 bins\"])\n",
    "    ax = plt.axes()\n",
    "    heatmap = sns.heatmap(mc_viz, annot=True, cmap=\"Blues\", vmin=0.50, vmax=1, ax = ax)\n",
    "    plt.title(\"Dataset with \" + type_dataframe.replace(\"_\",\" \"))\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "        \n",
    "    # adding these lists to the primary subgroups of lists\n",
    "    all_mc_train.append(df_mc_train)\n",
    "    all_mc_val.append(df_mc_val)\n",
    "    all_mc_test.append(df_mc_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Final visualization\"\"\"\n",
    "types_dataframes = [\"Dwell times between events\", \"Only events\", \"Only dwell times\",\n",
    "                       \"Mixed features\",\"Unrolled events\",\"Shuffled dwell times between events\"]\n",
    "\n",
    "lollipop_mc = pd.DataFrame(np.array(all_mc_test)[:,:,3], columns = [\"1 bin\", \"2 bins\", \"3 bins\", \"4 bins\"], index = [\"dataset 1\", \"dataset 2\", \"dataset 3\", \"dataset 4\", \"dataset5\", \"dataset 6\"])\n",
    "plt.hlines(y=range(6), xmin=lollipop_mc[\"1 bin\"], xmax=lollipop_mc[\"4 bins\"], color='grey', alpha = 0.5)\n",
    "plt.plot(lollipop_mc['1 bin'], types_dataframes, \"o\", alpha = 0)\n",
    "plt.scatter(lollipop_mc['4 bins'], types_dataframes, color='#00ce00', label='4 bins')\n",
    "plt.scatter(lollipop_mc['3 bins'], types_dataframes, color='#009a00', label='3 bins')\n",
    "plt.scatter(lollipop_mc['2 bins'], types_dataframes, color='#006700', label='2 bins')\n",
    "plt.scatter(lollipop_mc['1 bin'], types_dataframes, color='#003400', label='1 bin')\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1)\n",
    " \n",
    "# adding title and axis names\n",
    "plt.title(\"Accuracy with Markov Chains\", loc='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the essential parts and the results of the Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the last created dictionary of transition probabilities (no buy) in the loop (belonging to the shuffled dwell times with the events)\n",
    "print(\"no buy sequences, transition probability matrix\")\n",
    "all_count_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the last created dictionary of transition probabilities (buy) in the loop (belonging to the shuffled dwell times with the events)\n",
    "print(\"buy sequences, transition probability matrix\")\n",
    "all_count_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows that all the probabilities in the sequence sum 1 (or very close numbers)\n",
    "set([sum(type_dict[key].values()) for type_dict in all_count_dicts for key in type_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below save the results and show the raw numbers of the obtained accuracy\n",
    "np.savez_compressed(\"results_mc\", np.array(all_mc_test))\n",
    "print(\"raw results with dwell times between events\\n\", np.array(all_mc_test[0]), \"\\n\")\n",
    "print(\"raw results with only events\\n\", np.array(all_mc_test[1]), \"\\n\")\n",
    "print(\"raw results with only dwell times\\n\", np.array(all_mc_test[2]), \"\\n\")\n",
    "print(\"raw results with mixed features\\n\", np.array(all_mc_test[3]), \"\\n\")\n",
    "print(\"raw results with unrolled_events\\n\", np.array(all_mc_test[4]), \"\\n\")\n",
    "print(\"raw results with shuffled_dwell_times_and_events\\n\", np.array(all_mc_test[5]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_lstm = np.array(list(pd.read_csv('y.csv').iloc[:,0]))\n",
    "\n",
    "all_lstm_test = []\n",
    "for type_dataframe in [\"dwell_times_and_events\", \"only_events\", \"only_dwell_times\",\n",
    "                       \"mixed_features\",\"unrolled_events\",\"shuffled_dwell_times_and_events\"]:\n",
    "    lstm_scores_test = []\n",
    "    for k in range(1,5):\n",
    "        if type_dataframe != \"only_events\" or (type_dataframe == \"only_events\" and k == 1):\n",
    "            x_lstm = np.load(\"x_\" + str(k) + \"_\" + type_dataframe + \".npz\")[\"arr_0\"].astype(\"str\")\n",
    "            dict_keys = np.unique([x for session in x_lstm for x in session])\n",
    "            print(\"features\", dict_keys)\n",
    "            dict_values = np.array(pd.get_dummies(dict_keys))\n",
    "            lstm_dict = dict(zip(dict_keys, dict_values))\n",
    "            lstm_dict[\"0\"] = np.full(len(dict_keys), -100000)\n",
    "            ohe_x = []\n",
    "            for arr in np.array(x_lstm):\n",
    "                ohe_arr = np.array([lstm_dict[value][1:] for value in arr])\n",
    "                ohe_x.append(ohe_arr)\n",
    "            ohe_x = np.array(ohe_x)\n",
    "            print(ohe_x)\n",
    "\n",
    "            # train and the rest\n",
    "            x_lstm_train, x_lstm_testval, y_lstm_train, y_lstm_testval = train_test_split(ohe_x, y_lstm, stratify = y_lstm, test_size = 0.3, random_state= 333)\n",
    "            # train, val, test (0.3 * 0.5 is 0.15)\n",
    "            x_lstm_val, x_lstm_test, y_lstm_val, y_lstm_test = train_test_split(x_lstm_testval, y_lstm_testval, stratify = y_lstm_testval, test_size = 0.5, random_state= 400)\n",
    "            # checking number of features\n",
    "            print(x_lstm_train.shape)\n",
    "\n",
    "            n_batches = 10\n",
    "            n_epochs = 20\n",
    "            n_timesteps = x_lstm_train.shape[1]\n",
    "            n_features = x_lstm_train.shape[2]\n",
    "\n",
    "            # lstm with one-hot-encoding\n",
    "            lstm = Sequential()\n",
    "            lstm.add(LSTM(20, input_shape = (n_timesteps, n_features), return_sequences = True))\n",
    "            lstm.add(GlobalMaxPooling1D())\n",
    "            lstm.add(Dropout(0.5))\n",
    "            lstm.add(Dense(1, activation='sigmoid'))\n",
    "            np.random.seed(40)\n",
    "            adam = optimizers.Adam(lr=0.001)\n",
    "            lstm.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            print(lstm.summary())\n",
    "            np.random.seed(40)\n",
    "            history = lstm.fit(x_lstm_train, y_lstm_train, batch_size=n_batches, epochs=n_epochs, validation_data=(x_lstm_val, y_lstm_val), shuffle=True)\n",
    "\n",
    "            # accuracy on the val set \n",
    "            plt.plot(history.history['accuracy'])\n",
    "            plt.plot(history.history['val_accuracy'])\n",
    "            plt.title('model accuracy with ' + str(k) + \"bins in \" + type_dataframe)\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'val'], loc='upper left')\n",
    "            plt.show()\n",
    "            # loss on the val set\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss with ' + str(k) + \"bins in \" + type_dataframe)\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'val'], loc='upper left')\n",
    "            plt.show()\n",
    "            # accuracy on the test set \n",
    "            if type_dataframe != \"only_events\": \n",
    "                lstm_scores_test.append(lstm.evaluate(x_lstm_test, y_lstm_test, verbose=0))\n",
    "            else:\n",
    "                lstm_scores_test.extend([lstm.evaluate(x_lstm_test, y_lstm_test, verbose=0)]*4)\n",
    "    all_lstm_test.append(lstm_scores_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the essential parts and the results of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"results_lstm\", np.array(all_lstm_test))\n",
    "print(\"raw results with dwell times between events\", all_lstm_test[0], \"\\n\")\n",
    "print(\"raw results with only events\", all_lstm_test[1], \"\\n\")\n",
    "print(\"raw results with only dwell times\", all_lstm_test[2], \"\\n\")\n",
    "print(\"raw results with mixed features\", all_lstm_test[3], \"\\n\")\n",
    "print(\"raw results with unrolled_events\", all_lstm_test[4], \"\\n\")\n",
    "print(\"raw results with shuffled_dwell_times_and_events\", all_lstm_test[5], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Final visualization\"\"\"\n",
    "types_dataframes = [\"Dwell times between events\", \"Only events\", \"Only dwell times\",\n",
    "                       \"Mixed features\",\"Unrolled events\",\"Shuffled dwell times between events\"]\n",
    "\n",
    "lollipop_lstm = pd.DataFrame(np.array(all_lstm_test)[:,:,1], columns = [\"1 bin\", \"2 bins\", \"3 bins\", \"4 bins\"], index = [\"dataset 1\", \"dataset 2\", \"dataset 3\", \"dataset 4\", \"dataset5\", \"dataset 6\"])\n",
    "plt.hlines(y=range(6), xmin=lollipop_lstm[\"1 bin\"], xmax=lollipop_lstm[\"4 bins\"], color='grey', alpha = 0.5)\n",
    "plt.plot(lollipop_lstm['1 bin'], types_dataframes, \"o\", alpha = 0)\n",
    "plt.scatter(lollipop_lstm['4 bins'], types_dataframes, color='#00ce00', label='4 bins')\n",
    "plt.scatter(lollipop_lstm['3 bins'], types_dataframes, color='#009a00', label='3 bins')\n",
    "plt.scatter(lollipop_lstm['2 bins'], types_dataframes, color='#006700', label='2 bins')\n",
    "plt.scatter(lollipop_lstm['1 bin'], types_dataframes, color='#003400', label='1 bin')\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1)\n",
    " \n",
    "# adding title and axis names\n",
    "plt.title(\"Accuracy with LSTM\", loc='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
